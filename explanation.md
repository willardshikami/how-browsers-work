
### How Browsers Work
<dd>The first step in loading a web page is getting to the server. The browser takes the URL and extracts the host name from it and does the DNS query alert to get an IP address. The browser will use that IP address and use TCP to create a connection to it. Once it has that TCP connection, it makes a HTTP request to get that URL.</dd><br />
 <dd>Once the browser has the HTML, it starts going through it and builds an object in its memory called the **Document Object Memory (DOM)**. [The DOM is a tree structure and it has some specific rules. There is a HTML tag at the top and below it is a head tag and a body tag](https://www.ntu.edu.sg/home/ehchua/programming/java/J6d_xml.html) The Head tag is more declarative, it is meant to include things like the title of the page or maybe search engine keywords and all sorts of meta information on the page. The body element is where the content of the page lies, this might be the text of the page or image references. The DOM holds thousands of different elements and like the HTML, it mixes in content like images, text and structure things like layouts and scripts.</dd><br />
<dd>In reality, web pages do not really follow this clean head and body split. Looking at a live HTML you will often find content in head tags, or titles in body tags, and scripts or HTML pages with multiple head tags or multiple body tags and tags that do not close. Practically, nearly every web page has some form of bad HTML or badly formed HTML. Over the years, browsers have gotten used to that and have grown tolerant of these types of bad HTML. They use all sorts of logic and algorithms to clean up these HTMLs. If there are two heads, they just merge them into one, if there are multiple bodies they might merge those too and if a tag does not close, they make their best guess to try and implicitly close it. These correction are usually not standardized. So different browsers may correct the page differently.</dd><br />
<dd>As the browser constructs the DOM, it comes across many references to other resources, these are files like images or scripts that are needed to load the page and make it function. An average page today holds a lot of these resources and they add up to be about 1.5MBs in size, so quite significant. Once the browser sees those references in proceeds to download them. Because there are so many of them, to expedite the process browsers download many of them in parallel. Some of these resources are served from a different domain from the one the HTML was originally served from so we might see things like additional DNS resolutions or additional TCP connections in general.</dd><br /> 
<dd>Because there are so many resources and additional requirements in total, downloading these resources is the most time consuming part of loading most web pages often accounting to eighty or ninety percent of the time that it takes to load the page.These resources have three primary resource types;  **Style Sheets** or **CSS**, **Scripts** and **Images**.</dd><br />
<dd>**Scripts** are the most dynamic elements in the page, they can actually change the DOM dynamically and they do so quite often. Scripts are guaranteed to be the power in force behind any animation and interaction within the page like collapsing menus or sections that move around. They are also the primary way that third party components integrate on onto the page like a social like button. Scripts are both great and terrible on web pages.</dd><br />
<dd>**Style Sheet** or **CSS** files are another component and they are in charge of making the page pretty and usable. Styles dictate things like font size, layout and how you order content. They allow the split between text, images and content of the page and the design of it, making it easier to update the two separately.</dd><br />

<dd>**Images** are the most common resource on the page. They are very simple and don't do much apart from showing up, but they do compensate for that in volume. Images account for over half the request and even more of the bytes on page. They do have a big impact on the page load time.</dd><br />
<dd>Modern browsers do a lot in parallel but some file types do make them stop. The primary blocker is Java Script. A script is a very powerful player on a web page and it can make all sorts of changes to the DOM. Many of these changer are done after the page loads eg adding interactive elements but some changes can actually happen while the page is actually loading. The most specific function that does this is called document.write. It actually allows the scripts to inject content into the HTML as the browser is reading. These types of changes can dramatically modify the meaning of HTML. A script may inject an HTML comment and render everything below it to be a comment requiring the browser to ignore it. This is a drastic change and it can invalidate everything the browser created in the DOM beforehand. As a result, whenever browsers encounter scripts they actually need to stop building the DOM and execute the script. Since some scripts are external, this means that the browser has to wait until the script is downloaded and executed before it can continue parsing the HTML. These delays can have a very significant impact to the page load time.</dd><br />
<dd>Somewhere between downloading and building the DOM, the browser also needs to do what the user expected it to do which is paint the actual page. Two types of resources would delay or block the loading and rendering of the page. The first one is the style sheet which actually block rendering on purpose. There is no point of showing a jumble of images and text on the page without laying them out properly. So the browser will not paint anything on the page until all CSS files are fully downloaded and processed.<dd><br />
<dd>Scripts block the construction of DOM, because they do that, and because the browser can only paint what it understood and put it into the DOM, effectively, they also block rendering. They block the rendering of the content that is below where they are. This is especially noticeable with third party scripts that often represent a very small portion of the page like an advert or a little social button but they still block everything below them and when you see a page rendering in chunks and blocks this is what usually happens behind the scenes. These two behaviors also lead to the best practices of putting CSS files at the top where they can be downloaded early by browsers and putting JavaScript at the bottom where there is little real content below them and therefore they don't really block much.</dd><br />
<dd>For old browsers this process rend at the top, this means if they load a page that has two scripts on it, the first script will have to be downloaded and executed before the second one is even downloaded, this could make drastic changes to the web page making the second script non existent.</dd>
<dd>Newer browsers have sort of realized that while the first script might do something drastic it rarely actually does so. They added and additional parser called the **speculative parser** or also called the pre parser. This parser does not construct a DOM but rather just parses through the HTML and gathers information, it does not execute, it doesn't change anything. So it does not need to be stopped or delayed by scripts. With the information it finds, the pre parsers primary job is to load resources faster. In the case of two scripts, it would start downloading the second script even before the first script is downloaded and executed.  In other cases it may not download other resources but it would do other prep work like doing a DNS resolution or creating a TCP connection. This means that the resources are downloaded faster and with them the overall page.</dd><br /> 
<dd>As its name implies, the speculative parser is not always right. In certain cases, the first script may indeed navigate away from the page for instance downloading a desktop web page on a mobile device, a script may direct you to the m. website to see the mobile version of the site. In these cases, the speculative loading or downloading of web pages is wasteful, however in many cases the pre parser guesses it right, in general, web browsing has gotten much faster since the pre parser was introduced.</dd>
<dd>The pre parser gives us a lot of information. It lets us see a lot of resources on the web page. Although we see all of them, downloading them in parallel may not be a good thing. For instance, as mentioned earlier, CSS and JavaScript have a bigger impact byte for byte on the render time because of their render implications. Therefore, user experience might actually be better if the browser held off from downloading the images in favor of downloading the CSS and JavaScript files first.</dd> 
<dd>Another example is bandwidth contention. Trying to squeeze 50 different images down a certain download bandwidth, each of these 50 images would get only 2 percent of the bandwidth and would take a long time to load. If instead 6 images are downloaded at a time, these 6 images would download faster.</dd><br />
<dd>Modern browsers use all sorts of arbitrary rules often based on their own experiments to guide this behavior. Most of them would block image download until JavaScript and CSS files have been fetched. These rules are good but are often not ideal for a specific website's needs. As a result, various website techniques are  evolved to change the browser behavior basically to make it load the page in a slightly different way which is more tuned to this specific website's needs. 
These techniques generally fall under the **Front End Optimization umbrella**.</dd><br />
<dd>The browser needs to coordinate and download hundreds of different resources and it needs to process them. It does all this while striving for a fast user experience. While powerful browsers are still limited, they have to make trade-offs. They don't actually know what the resources holds until they download and see it. They are a one-size-fits all solution, they need to have a generic logic. Websites have the bonus and opportunity to optimize their own websites and guide the browser to provide the best  user experience for their specific websites.</dd>
